{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries & Data - RUN FIRST - Code below is dependent on these libraries and variables\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers as Optimizer\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialise Dataset\n",
    "X = [0.0354, 1.0331], [-0.1302, 1.3282], [0.1665, 0.6873], [1.3223, -.1472], [0.0643, -.0250], [-0.1051,.9224]\n",
    "Y = [.0, 1.0,0.0,.0],[.0,1.0,.0,.0], [.0,1.0,.0,.0], [.0,.0,1.0,.0], [1.0,.0,.0,.0], [.0,.0,.0,1.0]\n",
    "\n",
    "# Initialise Test Data\n",
    "testResult = []\n",
    "testDP = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm which tests each combination of hyper parameter. Used in my ANN evaluatuion\n",
    "\n",
    "# Hyper Parameters\n",
    "ACT = ['relu', 'sigmoid', 'tanh', 'elu' ]\n",
    "ACT_COUNT = len(ACT)\n",
    "\n",
    "LR = [ 0.1, 0.2, 0.3, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0 ]\n",
    "LR_COUNT = len(LR)\n",
    "\n",
    "LOSS = [ 'binary_crossentropy', 'mean_squared_error' , 'huber_loss', 'kullback_leibler_divergence'] \n",
    "LOSS_COUNT = len(LOSS)\n",
    "\n",
    "OPT = [ Optimizer.Adadelta, Optimizer.Adam, Optimizer.Adagrad, Optimizer.Nadam, Optimizer.RMSprop]\n",
    "OPT_COUNT = len(OPT)\n",
    "\n",
    "EPOCH = [100] # [25, 50, 100]\n",
    "EPOCH_COUNT = len(EPOCH)\n",
    "\n",
    "TEST_MAX = (ACT_COUNT ** 2) * LR_COUNT * OPT_COUNT * LOSS_COUNT * EPOCH_COUNT\n",
    "\n",
    "# Run tests for hyper parameter combinations\n",
    "for epoch_i in range(0, EPOCH_COUNT, 1):\n",
    "    for act_i in range(0, ACT_COUNT, 1):\n",
    "        for act_j in range(0, ACT_COUNT, 1):\n",
    "            for lr_i in range(0, LR_COUNT, 1):\n",
    "                for opt_i in range(0, OPT_COUNT,1):\n",
    "                    for loss_i in range(0, LOSS_COUNT,1):\n",
    "\n",
    "                        # Current Model\n",
    "                        model = Sequential()\n",
    "\n",
    "                        # Add the first layer (input layer)\n",
    "                        model.add(Dense(2, activation=ACT[act_i]))\n",
    "\n",
    "                        # Add a second layer (hidden layer)\n",
    "                        model.add(Dense(4, activation=ACT[act_i]))\n",
    "\n",
    "                        # Add the output layer\n",
    "                        model.add(Dense(4, activation=ACT[act_j]))\n",
    "\n",
    "                        # Compile the model\n",
    "                        model.compile(loss = LOSS[loss_i], optimizer = OPT[opt_i](learning_rate= LR[lr_i]), metrics = ['accuracy'])\n",
    "\n",
    "                        # Train the model with epochs, batch size, and validation data\n",
    "                        model.fit(X, Y, epochs=EPOCH[epoch_i], batch_size=32, verbose = 0)\n",
    "\n",
    "                        # Evaluate the model testing set and get the loss and accuracy\n",
    "                        loss, accuracy = model.evaluate(X,Y)\n",
    "\n",
    "                        # Save test results\n",
    "                        testResult.append({ \n",
    "                            'loss': round(loss, testDP),\n",
    "                            'accuracy': round(accuracy, testDP),\n",
    "                            'epochs': EPOCH[epoch_i],\n",
    "                            'act_hid': ACT[act_i],\n",
    "                            'act_out': ACT[act_j],\n",
    "                            'learn rate': LR[lr_i],\n",
    "                            'optimizer': model.optimizer.get_config()['name'],\n",
    "                        })\n",
    "                            \n",
    "                        print('Test {}/{} complete'.format(len(testResult), TEST_MAX))\n",
    "\n",
    "# Export test results to csv\n",
    "dateTimeString = datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "pd.DataFrame(testResult).to_csv('Test Hyper {}.csv'.format(dateTimeString), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm which tests for the best number of neurons and layers to use \n",
    "\n",
    "# Test Parameters\n",
    "layerMax = 5\n",
    "neuronMax = 21\n",
    "testMax = layerMax * neuronMax\n",
    "\n",
    "for l in range(1, layerMax, 1):\n",
    "    for n in range(2, neuronMax, 1):\n",
    "\n",
    "        # Create a Sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add our hidden layers\n",
    "        for i in range(0, l, 1):\n",
    "            model.add(Dense(n, activation='elu'))\n",
    "\n",
    "        # Add the output layer\n",
    "        model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "        # Compile the model\n",
    "        # We'll use the binary crossentropy loss function, the Adam optimizer, and accuracy as the metric\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Optimizer.Nadam(0.1), metrics=['accuracy'])\n",
    "\n",
    "        # Assuming X_train and y_train are your training data and labels\n",
    "        model.fit(X, Y, epochs=100, batch_size=32)\n",
    "\n",
    "        # Evaluate the model testing set and get the loss and accuracy\n",
    "        loss, accuracy = model.evaluate(X,Y)\n",
    "\n",
    "        testResult.append({'loss': round(loss, testDP), 'accuracy': round(accuracy, testDP), 'Layers': l, 'neurons': n})\n",
    "\n",
    "        print('Test {}/{} complete'.format(len(testResult), testMax))\n",
    "\n",
    "# Export test results to csv\n",
    "dateTimeString = datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "pd.DataFrame(testResult).to_csv('Test Neurons {}.csv'.format(dateTimeString), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Optimal Model\n",
    "neruonCount = 6\n",
    "layerCount = 2\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a second layer (hidden layer)\n",
    "for i in range(0, layerCount, 1):\n",
    "    model.add(Dense(6, activation='elu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(4, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "# We'll use the binary crossentropy loss function, the Adam optimizer, and accuracy as the metric\n",
    "model.compile(loss='binary_crossentropy', optimizer=Optimizer.Nadam(0.1), metrics=['accuracy'])\n",
    "\n",
    "# Assuming X_train and y_train are your training data and labels\n",
    "model.fit(X, Y, epochs=50, batch_size=32)\n",
    "\n",
    "loss, accuracy = model.evaluate(X,Y)\n",
    "\n",
    "print('Loss:', round(loss, testDP), 'Accuracy:', round(accuracy, testDP))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
